{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "tpu1vmV38",
   "dataSources": [
    {
     "sourceId": 7070871,
     "sourceType": "datasetVersion",
     "datasetId": 4071937
    }
   ],
   "dockerImageVersionId": 30589,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip install seaborn"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T08:12:06.157600Z",
     "iopub.execute_input": "2023-11-28T08:12:06.157848Z",
     "iopub.status.idle": "2023-11-28T08:12:10.697099Z",
     "shell.execute_reply.started": "2023-11-28T08:12:06.157821Z",
     "shell.execute_reply": "2023-11-28T08:12:10.696099Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install wordcloud"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T08:12:10.698476Z",
     "iopub.execute_input": "2023-11-28T08:12:10.698790Z",
     "iopub.status.idle": "2023-11-28T08:12:14.405536Z",
     "shell.execute_reply.started": "2023-11-28T08:12:10.698759Z",
     "shell.execute_reply": "2023-11-28T08:12:14.404591Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install plotly"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T08:12:14.408864Z",
     "iopub.execute_input": "2023-11-28T08:12:14.409182Z",
     "iopub.status.idle": "2023-11-28T08:12:28.688803Z",
     "shell.execute_reply.started": "2023-11-28T08:12:14.409150Z",
     "shell.execute_reply": "2023-11-28T08:12:28.687948Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install nltk\n",
    "!pip install spacy"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T08:12:28.690184Z",
     "iopub.execute_input": "2023-11-28T08:12:28.690778Z",
     "iopub.status.idle": "2023-11-28T08:12:44.969493Z",
     "shell.execute_reply.started": "2023-11-28T08:12:28.690740Z",
     "shell.execute_reply": "2023-11-28T08:12:44.968628Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import plotly.express as px\n",
    "from plotly.offline import init_notebook_mode\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "\n",
    "nltk.download('omw-1.4')\n",
    "tqdm.pandas()\n",
    "#spacy_eng = spacy.load(\"en_core_web_sm\")\n",
    "nltk.download('stopwords')\n",
    "lemm = WordNetLemmatizer()\n",
    "init_notebook_mode(connected=True)\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.rcParams['figure.figsize'] = (20,8)\n",
    "plt.rcParams['font.size'] = 18"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2023-11-28T08:12:44.970748Z",
     "iopub.execute_input": "2023-11-28T08:12:44.970998Z",
     "iopub.status.idle": "2023-11-28T08:13:11.951325Z",
     "shell.execute_reply.started": "2023-11-28T08:12:44.970971Z",
     "shell.execute_reply": "2023-11-28T08:13:11.950515Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Cleaning and EDA\n",
    "- Cleaning and preprocessing text data\n",
    "- Finding insights about sentence lengths and words present in them"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "data = pd.read_csv(\"/kaggle/input/questionsdata/train.csv\")\n",
    "data.head(10)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T08:13:11.952374Z",
     "iopub.execute_input": "2023-11-28T08:13:11.952644Z",
     "iopub.status.idle": "2023-11-28T08:13:13.696448Z",
     "shell.execute_reply.started": "2023-11-28T08:13:11.952616Z",
     "shell.execute_reply": "2023-11-28T08:13:13.695459Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Remove Null Values"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "data.isnull().sum()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T08:13:13.697769Z",
     "iopub.execute_input": "2023-11-28T08:13:13.698066Z",
     "iopub.status.idle": "2023-11-28T08:13:13.760376Z",
     "shell.execute_reply.started": "2023-11-28T08:13:13.698035Z",
     "shell.execute_reply": "2023-11-28T08:13:13.759351Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "data.dropna(inplace=True)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T08:13:13.761731Z",
     "iopub.execute_input": "2023-11-28T08:13:13.762049Z",
     "iopub.status.idle": "2023-11-28T08:13:13.849814Z",
     "shell.execute_reply.started": "2023-11-28T08:13:13.762016Z",
     "shell.execute_reply": "2023-11-28T08:13:13.848784Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Duplicate and Non-Duplicate Data Distribution\n",
    "- Duplicate or similar questions are lesser in number which is to be expected as a platform for question answering will tend to have more unique questions in comparison to the questions that have been previously asked"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "fig = px.pie(data, values='id', names='is_duplicate', height=600, title='Proportion of Duplicate and Non Duplicate Questions')\n",
    "fig.show()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T08:13:13.853196Z",
     "iopub.execute_input": "2023-11-28T08:13:13.853519Z",
     "iopub.status.idle": "2023-11-28T08:13:14.723190Z",
     "shell.execute_reply.started": "2023-11-28T08:13:13.853488Z",
     "shell.execute_reply": "2023-11-28T08:13:14.722247Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Text Cleaning\n",
    "- Since the context of sentences are important for this NLP problem removal of stopwords might affect both the grammatical as well as semantic meaning of the sentences\n",
    "- For similar reasons the words are not lemmatized or stemmed so that the semantic meaning of the sentence remains intact\n",
    "- Therefore very basic cleaning is performed on the text data like removal of extra spaces and special characters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def text_cleaning(x):\n",
    "    \n",
    "    questions = re.sub('\\s+\\n+', ' ', x)\n",
    "    questions = re.sub('[^a-zA-Z0-9]', ' ', questions)\n",
    "    questions = questions.lower()\n",
    "    \n",
    "    return questions"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T08:13:14.724315Z",
     "iopub.execute_input": "2023-11-28T08:13:14.724606Z",
     "iopub.status.idle": "2023-11-28T08:13:14.729185Z",
     "shell.execute_reply.started": "2023-11-28T08:13:14.724574Z",
     "shell.execute_reply": "2023-11-28T08:13:14.728430Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "data['question1_cleaned'] = data['question1'].progress_apply(text_cleaning)\n",
    "data['question2_cleaned'] = data['question2'].progress_apply(text_cleaning)\n",
    "data"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T08:13:14.730177Z",
     "iopub.execute_input": "2023-11-28T08:13:14.730444Z",
     "iopub.status.idle": "2023-11-28T08:13:20.912107Z",
     "shell.execute_reply.started": "2023-11-28T08:13:14.730418Z",
     "shell.execute_reply": "2023-11-28T08:13:20.911254Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sentence Length Distributions\n",
    "- Here the objective is to find the ideal length of the sentence that should be used in our model\n",
    "- In many cases the maximum sentence length is taken for embedding representations but by looking at the sentence length distributions a more informed decision can be made which will help in reducing the parameters of our model\n",
    "- For transformers based models masks usually mask out the sentences which are short but are padded to a longer length, but since we are focused on finding the ideal length statistically we will go ahead with that"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "data['question1_lens'] = data['question1_cleaned'].apply(lambda x: len(x.split()))\n",
    "data['question2_lens'] = data['question2_cleaned'].apply(lambda x: len(x.split()))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T08:13:20.913209Z",
     "iopub.execute_input": "2023-11-28T08:13:20.913501Z",
     "iopub.status.idle": "2023-11-28T08:13:21.756258Z",
     "shell.execute_reply.started": "2023-11-28T08:13:20.913473Z",
     "shell.execute_reply": "2023-11-28T08:13:21.755327Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "px.histogram(data, x=\"question1_lens\",height=700, color='is_duplicate', title=\"Question1 Length Distribution\", marginal=\"box\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T08:13:21.757389Z",
     "iopub.execute_input": "2023-11-28T08:13:21.757652Z",
     "iopub.status.idle": "2023-11-28T08:13:22.355548Z",
     "shell.execute_reply.started": "2023-11-28T08:13:21.757625Z",
     "shell.execute_reply": "2023-11-28T08:13:22.354610Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "px.histogram(data, x=\"question2_lens\",height=700, color='is_duplicate', title=\"Question2 Length Distribution\", marginal=\"box\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T08:13:22.357226Z",
     "iopub.execute_input": "2023-11-28T08:13:22.357501Z",
     "iopub.status.idle": "2023-11-28T08:13:22.951562Z",
     "shell.execute_reply.started": "2023-11-28T08:13:22.357473Z",
     "shell.execute_reply": "2023-11-28T08:13:22.950651Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Word Cloud Visualization\n",
    "- Word clouds help in visually identifying the most frequent words present in the sentences which also give a brief idea what the context of the sentences are\n",
    "- Two wordclouds are visualized below for both pairs of sentences "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "question1 = data['question1_cleaned'].tolist()\n",
    "question2 = data['question2_cleaned'].tolist()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T08:13:22.952784Z",
     "iopub.execute_input": "2023-11-28T08:13:22.953043Z",
     "iopub.status.idle": "2023-11-28T08:13:22.970799Z",
     "shell.execute_reply.started": "2023-11-28T08:13:22.953015Z",
     "shell.execute_reply": "2023-11-28T08:13:22.969940Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "wordcloud = WordCloud(max_words=1500, width=600, background_color='black').generate(\" \".join(question1))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.title(\"Words from Question1\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T08:13:22.971851Z",
     "iopub.execute_input": "2023-11-28T08:13:22.972108Z",
     "iopub.status.idle": "2023-11-28T08:13:35.211093Z",
     "shell.execute_reply.started": "2023-11-28T08:13:22.972081Z",
     "shell.execute_reply": "2023-11-28T08:13:35.210219Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "wordcloud = WordCloud(max_words=1500, width=600, background_color='black').generate(\" \".join(question2))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.title(\"Words from Question2\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T08:13:35.212151Z",
     "iopub.execute_input": "2023-11-28T08:13:35.212448Z",
     "iopub.status.idle": "2023-11-28T08:13:47.384953Z",
     "shell.execute_reply.started": "2023-11-28T08:13:35.212411Z",
     "shell.execute_reply": "2023-11-28T08:13:47.383961Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ideal Sentence Length\n",
    "- By looking at both the distrbution plots and descriptive statistics it is pretty clear that taking the maximum sentence length won't make much sense of our model\n",
    "- The descriptive stats also represent the likeliness of an extremely long sentence really occuring on a platform like Quora\n",
    "- Since the descriptive stats of both the pairs of questions look very similar lets analyse any one of them to find the upper outlier\n",
    "- Once this upper outlier is found we can choose a number nearby to it to be our ideal sentence length for our embedding representation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "data['question1_lens'].describe()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T08:13:47.386193Z",
     "iopub.execute_input": "2023-11-28T08:13:47.386546Z",
     "iopub.status.idle": "2023-11-28T08:13:47.408875Z",
     "shell.execute_reply.started": "2023-11-28T08:13:47.386511Z",
     "shell.execute_reply": "2023-11-28T08:13:47.407934Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "data['question2_lens'].describe()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T08:13:47.410236Z",
     "iopub.execute_input": "2023-11-28T08:13:47.410664Z",
     "iopub.status.idle": "2023-11-28T08:13:47.434501Z",
     "shell.execute_reply.started": "2023-11-28T08:13:47.410630Z",
     "shell.execute_reply": "2023-11-28T08:13:47.433579Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "q1 = data['question1_lens'].quantile(0.25)\n",
    "q3 = data['question1_lens'].quantile(0.75)\n",
    "\n",
    "upper_outlier = q3 + 1.5*(q3-q1)\n",
    "print(upper_outlier)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T08:13:47.435824Z",
     "iopub.execute_input": "2023-11-28T08:13:47.436236Z",
     "iopub.status.idle": "2023-11-28T08:13:47.457634Z",
     "shell.execute_reply.started": "2023-11-28T08:13:47.436193Z",
     "shell.execute_reply": "2023-11-28T08:13:47.456655Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install datasets"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T08:13:47.458721Z",
     "iopub.execute_input": "2023-11-28T08:13:47.459018Z",
     "iopub.status.idle": "2023-11-28T08:13:58.141653Z",
     "shell.execute_reply.started": "2023-11-28T08:13:47.458987Z",
     "shell.execute_reply": "2023-11-28T08:13:58.140440Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Inference:** Upper outlier is 22, lets take 50 to be the ideal length so that some of the extremely long sentences can also be represented well enough for our model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Embedding, Layer, Dense, Dropout, MultiHeadAttention, LayerNormalization, Input, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer,DataCollatorWithPadding,TFAutoModel,DistilBertConfig,TFDistilBertModel, BertConfig, TFBertModel, TFRobertaModel\n",
    "from datasets import load_dataset"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T08:13:58.143162Z",
     "iopub.execute_input": "2023-11-28T08:13:58.143483Z",
     "iopub.status.idle": "2023-11-28T08:14:12.885746Z",
     "shell.execute_reply.started": "2023-11-28T08:13:58.143448Z",
     "shell.execute_reply": "2023-11-28T08:14:12.884864Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model_checkpoint = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T08:14:12.886831Z",
     "iopub.execute_input": "2023-11-28T08:14:12.887420Z",
     "iopub.status.idle": "2023-11-28T08:14:13.918377Z",
     "shell.execute_reply.started": "2023-11-28T08:14:12.887384Z",
     "shell.execute_reply": "2023-11-28T08:14:13.917591Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BERT Text Tokenizer\n",
    "- Generates\n",
    "    - Padded Encodings\n",
    "    - Attention Masks"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def encode_text(text, tokenizer):\n",
    "    \n",
    "    encoded = tokenizer.batch_encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=50,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=\"tf\",\n",
    "    )\n",
    "\n",
    "    input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n",
    "    attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_masks\": attention_masks\n",
    "    }"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T08:14:13.919435Z",
     "iopub.execute_input": "2023-11-28T08:14:13.919731Z",
     "iopub.status.idle": "2023-11-28T08:14:13.924937Z",
     "shell.execute_reply.started": "2023-11-28T08:14:13.919700Z",
     "shell.execute_reply": "2023-11-28T08:14:13.924326Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Splitting\n",
    "- 400000 data is sampled for our task\n",
    "- 80:20 split is performed on the data\n",
    "    - 80% for Training\n",
    "    - 20% for Validation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "data = data.sample(400000)\n",
    "train = data.iloc[:int(400000*0.80),:]\n",
    "val = data.iloc[int(400000*0.80):,:]\n",
    "\n",
    "X1_train = encode_text(train['question1_cleaned'].tolist(), tokenizer)\n",
    "X2_train = encode_text(train['question2_cleaned'].tolist(), tokenizer)\n",
    "X1_val = encode_text(val['question1_cleaned'].tolist(), tokenizer)\n",
    "X2_val = encode_text(val['question2_cleaned'].tolist(), tokenizer)\n",
    "\n",
    "y_train = train['is_duplicate'].values\n",
    "y_val = val['is_duplicate'].values"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T08:14:13.925760Z",
     "iopub.execute_input": "2023-11-28T08:14:13.925984Z",
     "iopub.status.idle": "2023-11-28T08:14:50.601874Z",
     "shell.execute_reply.started": "2023-11-28T08:14:13.925960Z",
     "shell.execute_reply": "2023-11-28T08:14:50.600713Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TPU Configuration"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "    BATCH_SIZE = strategy.num_replicas_in_sync * 4\n",
    "    print(\"Running on TPU:\", tpu.master())\n",
    "    print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "    \n",
    "except ValueError:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    BATCH_SIZE = 32\n",
    "    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n",
    "    print(f\"Batch Size: {BATCH_SIZE}\")     "
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T08:14:50.602938Z",
     "iopub.execute_input": "2023-11-28T08:14:50.603237Z",
     "iopub.status.idle": "2023-11-28T08:14:55.159349Z",
     "shell.execute_reply.started": "2023-11-28T08:14:50.603207Z",
     "shell.execute_reply": "2023-11-28T08:14:55.158497Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class L1Dist(Layer):\n",
    "    \n",
    "    def __init__(self,**kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "    def call(self,embedding1,embedding2):\n",
    "        return tf.math.abs(embedding1 - embedding2)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T08:14:55.164353Z",
     "iopub.execute_input": "2023-11-28T08:14:55.164633Z",
     "iopub.status.idle": "2023-11-28T08:14:55.169824Z",
     "shell.execute_reply.started": "2023-11-28T08:14:55.164602Z",
     "shell.execute_reply": "2023-11-28T08:14:55.169058Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "with strategy.scope():\n",
    "    transformer_model = TFBertModel.from_pretrained(model_checkpoint)\n",
    "\n",
    "    input_ids_in1 = Input(shape=(None,),name='input_ids1', dtype='int32')\n",
    "    input_masks_in1 = Input(shape=(None,), name='attention_mask1', dtype='int32')\n",
    "    input_ids_in2 = Input(shape=(None,),name='input_ids2', dtype='int32')\n",
    "    input_masks_in2 = Input(shape=(None,), name='attention_mask2', dtype='int32')\n",
    "\n",
    "    embedding_layer1 = transformer_model(input_ids_in1, attention_mask=input_masks_in1).last_hidden_state\n",
    "    embedding_layer2 = transformer_model(input_ids_in2, attention_mask=input_masks_in2).last_hidden_state\n",
    "\n",
    "    embedding1 = GlobalAveragePooling1D()(embedding_layer1)\n",
    "    embedding2 = GlobalAveragePooling1D()(embedding_layer2)\n",
    "    l1_dist = L1Dist()(embedding1,embedding2)\n",
    "\n",
    "    x = Dense(512, activation='relu')(l1_dist)\n",
    "    output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=[input_ids_in1, input_masks_in1, input_ids_in2, input_masks_in2], outputs = output)\n",
    "    model.compile(loss='binary_crossentropy',optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001),metrics='accuracy')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T08:14:55.171592Z",
     "iopub.execute_input": "2023-11-28T08:14:55.171847Z",
     "iopub.status.idle": "2023-11-28T08:15:19.224471Z",
     "shell.execute_reply.started": "2023-11-28T08:14:55.171820Z",
     "shell.execute_reply": "2023-11-28T08:15:19.223627Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for layer in model.layers[:5]:\n",
    "    layer.trainable = False"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T08:15:19.225500Z",
     "iopub.execute_input": "2023-11-28T08:15:19.225795Z",
     "iopub.status.idle": "2023-11-28T08:15:19.234828Z",
     "shell.execute_reply.started": "2023-11-28T08:15:19.225765Z",
     "shell.execute_reply": "2023-11-28T08:15:19.234185Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T08:15:19.235767Z",
     "iopub.execute_input": "2023-11-28T08:15:19.236026Z",
     "iopub.status.idle": "2023-11-28T08:15:19.283359Z",
     "shell.execute_reply.started": "2023-11-28T08:15:19.235998Z",
     "shell.execute_reply": "2023-11-28T08:15:19.282729Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T08:15:19.284073Z",
     "iopub.execute_input": "2023-11-28T08:15:19.284323Z",
     "iopub.status.idle": "2023-11-28T08:15:19.287753Z",
     "shell.execute_reply.started": "2023-11-28T08:15:19.284297Z",
     "shell.execute_reply": "2023-11-28T08:15:19.287079Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "earlystopping = EarlyStopping(monitor='val_loss',min_delta = 0, patience = 5, verbose = 1, restore_best_weights=True)\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', \n",
    "                                            patience=3, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.3, \n",
    "                                            min_lr=0.00000001)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T08:15:19.288631Z",
     "iopub.execute_input": "2023-11-28T08:15:19.288883Z",
     "iopub.status.idle": "2023-11-28T08:15:19.297324Z",
     "shell.execute_reply.started": "2023-11-28T08:15:19.288858Z",
     "shell.execute_reply": "2023-11-28T08:15:19.296627Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "history = model.fit((np.asarray(X1_train['input_ids']),np.asarray(X1_train['attention_masks']),np.asarray(X2_train['input_ids']),np.asarray(X2_train['attention_masks'])), \n",
    "                    y_train, batch_size=BATCH_SIZE, epochs=15,  \n",
    "                    validation_data=((np.asarray(X1_val['input_ids']),np.asarray(X1_val['attention_masks']),np.asarray(X2_val['input_ids']),np.asarray(X2_val['attention_masks'])), y_val),\n",
    "                    callbacks=[earlystopping, learning_rate_reduction])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T08:15:19.298215Z",
     "iopub.execute_input": "2023-11-28T08:15:19.298473Z",
     "iopub.status.idle": "2023-11-28T10:02:30.032772Z",
     "shell.execute_reply.started": "2023-11-28T08:15:19.298448Z",
     "shell.execute_reply": "2023-11-28T10:02:30.031679Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model.save(\"model.h5\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T10:02:30.033826Z",
     "iopub.execute_input": "2023-11-28T10:02:30.034089Z",
     "iopub.status.idle": "2023-11-28T10:02:33.275882Z",
     "shell.execute_reply.started": "2023-11-28T10:02:30.034061Z",
     "shell.execute_reply": "2023-11-28T10:02:33.274757Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Inference\n",
    "- Learning Curves\n",
    "- ROC-AUC Curves\n",
    "- Confusion Matrix\n",
    "- Classification Report"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Learning Curve\n",
    "- Since it is a pretrained model we do not train it for longer epochs, also due to the fact that the validation loss increases after first 3 epochs the training is restricted to 5 epochs\n",
    "- The best weights from the most converged state are taken forward"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T12:09:09.865493Z",
     "iopub.execute_input": "2023-11-28T12:09:09.865872Z",
     "iopub.status.idle": "2023-11-28T12:09:10.182361Z",
     "shell.execute_reply.started": "2023-11-28T12:09:09.865840Z",
     "shell.execute_reply": "2023-11-28T12:09:10.181459Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T12:09:14.702882Z",
     "iopub.execute_input": "2023-11-28T12:09:14.703321Z",
     "iopub.status.idle": "2023-11-28T12:09:15.010684Z",
     "shell.execute_reply.started": "2023-11-28T12:09:14.703190Z",
     "shell.execute_reply": "2023-11-28T12:09:15.009656Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#questions for predictions\n",
    "sentence1 = \"How can I learn Python quickly?\"\n",
    "sentence2 = \"What are the best ways to learn bike quickly.\"\n",
    "\n",
    "#cleaning\n",
    "sentence1cleaned = text_cleaning(sentence1)\n",
    "sentence2cleaned = text_cleaning(sentence2)\n",
    "\n",
    "# Tokenize and encode the sentences\n",
    "tokens1 = tokenizer.encode(sentence1cleaned, add_special_tokens=True)\n",
    "tokens2 = tokenizer.encode(sentence2cleaned, add_special_tokens=True)\n",
    "\n",
    "# Assuming tokens1 and tokens2 are tokenized sequences\n",
    "# Construct inputs with attention masks\n",
    "Q1_val = {\n",
    "    'input_ids': [tokens1],\n",
    "    'attention_masks': [[1] * len(tokens1)],  # Considering all tokens in sentence 1\n",
    "}\n",
    "Q2_val = {\n",
    "    'input_ids': [tokens2],\n",
    "    'attention_masks': [[1] * len(tokens2)]   # Considering all tokens in sentence 2\n",
    "}\n",
    "\n",
    "# Model prediction for similarity score\n",
    "similarity_score = model.predict((np.asarray(Q1_val['input_ids']),np.asarray(Q1_val['attention_masks']),np.asarray(Q2_val['input_ids']),np.asarray(Q2_val['attention_masks'])))\n",
    "\n",
    "#similarity score\n",
    "print(f\"Similarity Score: {similarity_score[0][0]}\")\n",
    "\n",
    "#classify\n",
    "if similarity_score[0][0] < 0.5:\n",
    "    print(\"Dissimilar\")\n",
    "else:\n",
    "    print(\"similar\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T12:24:59.567638Z",
     "iopub.execute_input": "2023-11-28T12:24:59.567976Z",
     "iopub.status.idle": "2023-11-28T12:25:00.086109Z",
     "shell.execute_reply.started": "2023-11-28T12:24:59.567947Z",
     "shell.execute_reply": "2023-11-28T12:25:00.085028Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "y_pred = model.predict((np.asarray(X1_val['input_ids']),np.asarray(X1_val['attention_masks']),np.asarray(X2_val['input_ids']),np.asarray(X2_val['attention_masks'])))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T12:04:50.617644Z",
     "iopub.execute_input": "2023-11-28T12:04:50.618019Z",
     "iopub.status.idle": "2023-11-28T12:05:26.389645Z",
     "shell.execute_reply.started": "2023-11-28T12:04:50.617986Z",
     "shell.execute_reply": "2023-11-28T12:05:26.388487Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, recall_score"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T12:05:35.954424Z",
     "iopub.execute_input": "2023-11-28T12:05:35.954753Z",
     "iopub.status.idle": "2023-11-28T12:05:35.959174Z",
     "shell.execute_reply.started": "2023-11-28T12:05:35.954726Z",
     "shell.execute_reply": "2023-11-28T12:05:35.958324Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ROC-AUC Curve\n",
    "- The AUC score of 95% gives a clear indication about the good separability performance of our model \n",
    "- The threshold values can be experimented with to acheive the desirable number of True positives or avoiding False positives"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "fpr, tpr, _ = roc_curve(y_val,  y_pred)\n",
    "auc = roc_auc_score(y_val, y_pred)\n",
    "plt.plot(fpr,tpr,label=\"CNN Model, auc=\"+str(auc),lw=2)\n",
    "plt.plot([0, 1], [0, 1], color=\"orange\", lw=2, linestyle=\"--\")\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T12:05:40.243549Z",
     "iopub.execute_input": "2023-11-28T12:05:40.243994Z",
     "iopub.status.idle": "2023-11-28T12:05:40.506667Z",
     "shell.execute_reply.started": "2023-11-28T12:05:40.243955Z",
     "shell.execute_reply": "2023-11-28T12:05:40.505614Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Confusion Matrix"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "y_pred[y_pred>=0.5] = 1\n",
    "y_pred[y_pred<0.5] = 0\n",
    "\n",
    "sns.heatmap(confusion_matrix(y_val, y_pred),cmap='viridis',annot=True,fmt='.5g',\n",
    "            xticklabels=['Dissimilar','Similar'],yticklabels=['Dissimilar','Similar'])\n",
    "plt.xlabel('Predicted Class')\n",
    "plt.ylabel('Actual Class')\n",
    "plt.show()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T12:05:44.872021Z",
     "iopub.execute_input": "2023-11-28T12:05:44.872367Z",
     "iopub.status.idle": "2023-11-28T12:05:45.116937Z",
     "shell.execute_reply.started": "2023-11-28T12:05:44.872336Z",
     "shell.execute_reply": "2023-11-28T12:05:45.115911Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Classification Report\n",
    "- Our model achieves an F1-score of 89%\n",
    "- F1-score is considered as there is a slight imbalance in the data\n",
    "- Model performs slightlty less accurate for similar classes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "print(classification_report(y_val,y_pred))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T12:05:50.541835Z",
     "iopub.execute_input": "2023-11-28T12:05:50.542168Z",
     "iopub.status.idle": "2023-11-28T12:05:50.631028Z",
     "shell.execute_reply.started": "2023-11-28T12:05:50.542140Z",
     "shell.execute_reply": "2023-11-28T12:05:50.630089Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Preparation\n",
    "- The cleaned texts are rearranged and prepared in the anchor, positve and negative format"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "data = pd.read_csv(\"../input/triplet-data/triplet_data.csv\")\n",
    "data.head(10)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T10:03:22.466450Z",
     "iopub.status.idle": "2023-11-28T10:03:22.466744Z",
     "shell.execute_reply.started": "2023-11-28T10:03:22.466597Z",
     "shell.execute_reply": "2023-11-28T10:03:22.466612Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "len(data)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T10:03:22.467741Z",
     "iopub.status.idle": "2023-11-28T10:03:22.468051Z",
     "shell.execute_reply.started": "2023-11-28T10:03:22.467897Z",
     "shell.execute_reply": "2023-11-28T10:03:22.467913Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Splitting\n",
    "- 140000 samples are taken for this model\n",
    "- 80:20 splitting is performed\n",
    "    - 80% taken for training\n",
    "    - 20% taken for validation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "train = data.iloc[:int(140000*0.80),:]\n",
    "val = data.iloc[int(140000*0.80):,:]\n",
    "\n",
    "X1_train = encode_text(train['question1_cleaned'].tolist(), tokenizer)\n",
    "X2_train = encode_text(train['question2_cleaned'].tolist(), tokenizer)\n",
    "X3_train = encode_text(train['question3_cleaned'].tolist(), tokenizer)\n",
    "\n",
    "X1_val = encode_text(val['question1_cleaned'].tolist(), tokenizer)\n",
    "X2_val = encode_text(val['question2_cleaned'].tolist(), tokenizer)\n",
    "X3_val = encode_text(val['question3_cleaned'].tolist(), tokenizer)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T10:03:22.469023Z",
     "iopub.status.idle": "2023-11-28T10:03:22.469382Z",
     "shell.execute_reply.started": "2023-11-28T10:03:22.469195Z",
     "shell.execute_reply": "2023-11-28T10:03:22.469212Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Custom Distance Layer\n",
    "- calculates distane between anchor and positive and anchor and negative"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class DistanceLayer(Layer):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, anchor, positive, negative):\n",
    "        ap_distance = tf.reduce_sum(tf.square(anchor - positive), -1)\n",
    "        an_distance = tf.reduce_sum(tf.square(anchor - negative), -1)\n",
    "        return (ap_distance, an_distance)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T10:03:22.470176Z",
     "iopub.status.idle": "2023-11-28T10:03:22.470519Z",
     "shell.execute_reply.started": "2023-11-28T10:03:22.470353Z",
     "shell.execute_reply": "2023-11-28T10:03:22.470369Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from tensorflow.keras import metrics"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T10:03:22.471872Z",
     "iopub.status.idle": "2023-11-28T10:03:22.472164Z",
     "shell.execute_reply.started": "2023-11-28T10:03:22.472026Z",
     "shell.execute_reply": "2023-11-28T10:03:22.472041Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Custom Model\n",
    "- Takes the siamese network architecture as an input and optimizes it with respect to the triplet loss"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class SiameseModel(Model):\n",
    "\n",
    "    def __init__(self, siamese_network, margin=0.5):\n",
    "        super(SiameseModel, self).__init__()\n",
    "        self.siamese_network = siamese_network\n",
    "        self.margin = margin\n",
    "        self.loss_tracker = metrics.Mean(name=\"loss\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.siamese_network(inputs)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self._compute_loss(data)\n",
    "            \n",
    "        gradients = tape.gradient(loss, self.siamese_network.trainable_weights)\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(gradients, self.siamese_network.trainable_weights)\n",
    "        )\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        loss = self._compute_loss(data)\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def _compute_loss(self, data):\n",
    "        ap_distance, an_distance = self.siamese_network(data)\n",
    "        loss = ap_distance - an_distance\n",
    "        loss = tf.maximum(loss + self.margin, 0.0)\n",
    "        return loss\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_tracker]"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T10:03:22.473047Z",
     "iopub.status.idle": "2023-11-28T10:03:22.473391Z",
     "shell.execute_reply.started": "2023-11-28T10:03:22.473203Z",
     "shell.execute_reply": "2023-11-28T10:03:22.473219Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "with strategy.scope():\n",
    "    transformer_model = TFBertModel.from_pretrained(model_checkpoint)\n",
    "    \n",
    "    input_ids_in1 = Input(shape=(50,),name='input_ids1', dtype='int32')\n",
    "    input_masks_in1 = Input(shape=(50,), name='attention_mask1', dtype='int32')\n",
    "\n",
    "    anchor_input = Input(name=\"anchor_ids\", shape=(50,), dtype='int32')\n",
    "    anchor_masks = Input(name=\"anchor_mask\", shape=(50,), dtype='int32')\n",
    "\n",
    "    positive_input = Input(name=\"positive_ids\", shape=(50,), dtype='int32')\n",
    "    positive_masks = Input(name=\"positive_mask\", shape=(50,), dtype='int32')\n",
    "\n",
    "    negative_input = Input(name=\"negative_ids\", shape=(50,), dtype='int32')\n",
    "    negative_masks = Input(name=\"negative_mask\", shape=(50,), dtype='int32')\n",
    "\n",
    "    embedding_layer = transformer_model(input_ids_in1, attention_mask=input_masks_in1).last_hidden_state\n",
    "\n",
    "    average = GlobalAveragePooling1D()(embedding_layer)\n",
    "    embeds = Dense(512,activation='relu')(average)\n",
    "    \n",
    "    embeddings = Model(inputs=[input_ids_in1,input_masks_in1],outputs=embeds)\n",
    "    \n",
    "    for layer in embeddings.layers[:-1]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    embeds1 = embeddings([anchor_input,anchor_masks])\n",
    "    embeds2 = embeddings([positive_input,positive_masks])\n",
    "    embeds3 = embeddings([negative_input,negative_masks])\n",
    "\n",
    "    distances = DistanceLayer()(embeds1,embeds2,embeds3)\n",
    "    \n",
    "    siamese_network = Model(\n",
    "        inputs=[anchor_input, anchor_masks, positive_input, positive_masks, negative_input, negative_masks], outputs=distances\n",
    "    )\n",
    "\n",
    "    siamese_model = SiameseModel(siamese_network)\n",
    "    siamese_model.compile(optimizer=tf.keras.optimizers.Adam(0.00001))\n",
    "    history = siamese_model.fit((np.asarray(X1_train['input_ids']),np.asarray(X1_train['attention_masks']),\n",
    "                                   np.asarray(X2_train['input_ids']),np.asarray(X2_train['attention_masks']),\n",
    "                                   np.asarray(X3_train['input_ids']),np.asarray(X3_train['attention_masks'])), \n",
    "                                  epochs=10, \n",
    "                                  validation_data=((np.asarray(X1_val['input_ids']),np.asarray(X1_val['attention_masks']),\n",
    "                                   np.asarray(X2_val['input_ids']),np.asarray(X2_val['attention_masks']),\n",
    "                                   np.asarray(X3_val['input_ids']),np.asarray(X3_val['attention_masks'])),))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T10:03:22.474692Z",
     "iopub.status.idle": "2023-11-28T10:03:22.475011Z",
     "shell.execute_reply.started": "2023-11-28T10:03:22.474870Z",
     "shell.execute_reply": "2023-11-28T10:03:22.474885Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Inference\n",
    "- Learning Curve\n",
    "- Cosine Similarity between Embeddings"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Learning Curve\n",
    "- The model converges well, but the validation loss doesn't seem to be close to the training loss"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T10:03:22.475777Z",
     "iopub.status.idle": "2023-11-28T10:03:22.476052Z",
     "shell.execute_reply.started": "2023-11-28T10:03:22.475915Z",
     "shell.execute_reply": "2023-11-28T10:03:22.475930Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def get_cosine_similarity(sentence1, sentence2):\n",
    "    \n",
    "    x1 = text_cleaning(sentence1)\n",
    "    x1 = encode_text([x1], tokenizer)\n",
    "    x2 = text_cleaning(sentence2)\n",
    "    x2 = encode_text([x2], tokenizer)\n",
    "    \n",
    "    x1_inputs = np.array(x1['input_ids'])\n",
    "    x1_masks = np.array(x1['attention_masks'])\n",
    "    x2_inputs = np.array(x2['input_ids'])\n",
    "    x2_masks = np.array(x2['attention_masks'])\n",
    "\n",
    "    embeddings1 = embeddings([x1_inputs,x1_masks])\n",
    "    embeddings2 = embeddings([x2_inputs,x2_masks])\n",
    "    \n",
    "    cosine_similarity = metrics.CosineSimilarity()\n",
    "    \n",
    "    return cosine_similarity(embeddings1,embeddings2).numpy()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T10:03:22.476925Z",
     "iopub.status.idle": "2023-11-28T10:03:22.477200Z",
     "shell.execute_reply.started": "2023-11-28T10:03:22.477064Z",
     "shell.execute_reply": "2023-11-28T10:03:22.477078Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Sample Test Cases\n- Cosine Similarity ranges from 0 to 1. \n- Value closer to 1 indicates higher similarity and a value closer to 0 indicates dissimilarity",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "sentence1 = 'Is Earth circle in shape ?'\nsentence2 = 'Should I learn python as it is very popular ?'\nget_cosine_similarity(sentence1,sentence2)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T10:03:22.477876Z",
     "iopub.status.idle": "2023-11-28T10:03:22.478146Z",
     "shell.execute_reply.started": "2023-11-28T10:03:22.478011Z",
     "shell.execute_reply": "2023-11-28T10:03:22.478025Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "sentence1 = 'Python is one of the most popular programming language out there'\nsentence2 = 'Should I learn python programming as it is very popular ?'\nget_cosine_similarity(sentence1,sentence2)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T10:03:22.478871Z",
     "iopub.status.idle": "2023-11-28T10:03:22.479159Z",
     "shell.execute_reply.started": "2023-11-28T10:03:22.479011Z",
     "shell.execute_reply": "2023-11-28T10:03:22.479025Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "sentence1 = 'Which GPU gives a better performance NVIDIA or AMD ?'\nsentence2 = 'What is the recipe for Kolkata Chicken Roll?'\nget_cosine_similarity(sentence1,sentence2)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T10:03:22.480102Z",
     "iopub.status.idle": "2023-11-28T10:03:22.480402Z",
     "shell.execute_reply.started": "2023-11-28T10:03:22.480239Z",
     "shell.execute_reply": "2023-11-28T10:03:22.480254Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "sentence1 = 'Which GPU gives a better performance NVIDIA or AMD ?'\nsentence2 = 'My friend has a NVIDIA GPU, and he suggests that it gives a very smooth gaming performance'\nget_cosine_similarity(sentence1,sentence2)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T10:03:22.481185Z",
     "iopub.status.idle": "2023-11-28T10:03:22.481488Z",
     "shell.execute_reply.started": "2023-11-28T10:03:22.481339Z",
     "shell.execute_reply": "2023-11-28T10:03:22.481354Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "sentence1 = 'Which GPU gives a better performance NVIDIA or AMD ?'\nsentence2 = 'NVIDIA manufactures the best performing GPUS'\nget_cosine_similarity(sentence1,sentence2)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-28T10:03:22.482165Z",
     "iopub.status.idle": "2023-11-28T10:03:22.482482Z",
     "shell.execute_reply.started": "2023-11-28T10:03:22.482329Z",
     "shell.execute_reply": "2023-11-28T10:03:22.482344Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<div class='alert alert-success'><strong>Conclusion:</strong>\n    <li>Both the architectures seem to be performing well in terms of their inferences</li>\n    <li>Original Siamese network can be evalutated in terms of classification metrics and similar sentences can be found using <strong>probability scores</strong></li>\n    <li>Siamese network with triplet loss can be evaluated in terms of cosine distances and similar sentences can be found using <strong>higher cosine distances</strong></li>\n    <li>For a real time scenario a metric like <strong>Hit Rate or User Engagement</strong> will prove to be more useful to get an infication of the usability of the model</li>\n</div>",
   "metadata": {}
  }
 ]
}